{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER and POS-Tagging.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3GXzZ5bRQ8a"
      },
      "source": [
        "# **Stanza for English NER**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uzo8pyVzU13E",
        "collapsed": true
      },
      "source": [
        "!pip install stanza\n",
        "!pip install stanza-batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yjby4p2nXGLP",
        "collapsed": true
      },
      "source": [
        "import stanza\n",
        "import pandas as pd\n",
        "from stanza.models.common.doc import Document\n",
        "from typing import List\n",
        "from stanza_batch import batch\n",
        "stanza.download(lang='en', processors='tokenize,ner')\n",
        "nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def countFiles():\n",
        "  file_count = dict.fromkeys(list(range(1,18)), 0)\n",
        "  for ls in range(1,18):\n",
        "    files = 0\n",
        "    for file in range(1,1000):\n",
        "      filepath = '/content/drive/MyDrive/Lok Sabha Data/textfiles/' + str(ls) + '/' + str(file) + '.csv'\n",
        "      if os.path.isfile(filepath):\n",
        "        files += 1\n",
        "    file_count[ls] = files\n",
        "  return file_count\n",
        "  "
      ],
      "metadata": {
        "id": "ACTLOFKrnu67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "countFiles()"
      ],
      "metadata": {
        "id": "Tsxsz3-goFGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6ME_-fzFz6d"
      },
      "source": [
        "for ls in range(1, 18):\n",
        "    for file in range(92, 101):\n",
        "        stanza_documents: List[Document] = []\n",
        "        text_filename = '/content/drive/MyDrive/Lok Sabha Data/textfiles/' + str(ls_num) + '/' + str(file) + '.csv'\n",
        "        try:\n",
        "            textfile = pd.read_csv(text_filename, lineterminator='\\n', names=['index','speech'])\n",
        "        except FileNotFoundError as e:\n",
        "            continue\n",
        "        if textfile['speech'].isnull().all():\n",
        "            print(str(ls_num) + '-' + str(file) + ' is empty!')\n",
        "        else:\n",
        "            speech = [x for x in textfile['speech'].values if x]\n",
        "            for document in batch(speech, nlp, batch_size=32):\n",
        "                stanza_documents.append(document)\n",
        "            ner = []\n",
        "            for i in range(len(stanza_documents)):\n",
        "                for j in range(len(stanza_documents[i].sentences)):\n",
        "                for k in range(len(stanza_documents[i].sentences[j].ents)):\n",
        "                    ner.append([i-1, j, k, stanza_documents[i].sentences[j].ents[k].text, stanza_documents[i].sentences[j].ents[k].type]) \n",
        "            NER = pd.DataFrame(ner, columns=['speech#', 'sent#', 'ent#', 'entity_text', 'entity_type'])\n",
        "            NER_filename = '/content/drive/MyDrive/lok_sabha_data/NER/' + str(ls_num) + '/' + str(file) + '_NER.csv'\n",
        "            NER.to_csv(NER_filename)\n",
        "            print(str(ls_num) + '-' + str(file) + ' NER done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFE2aebARBng"
      },
      "source": [
        "# **Stanza for Hindi POS-tagging**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIa0IKSbRdHI"
      },
      "source": [
        "!pip install stanza\n",
        "!pip install stanza-batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnVOgB44RdHM"
      },
      "source": [
        "import stanza\n",
        "import pandas as pd\n",
        "from stanza.models.common.doc import Document\n",
        "from typing import List\n",
        "from stanza_batch import batch\n",
        "stanza.download(lang='hi', processors='tokenize,pos')\n",
        "nlp = stanza.Pipeline(lang='hi', processors='tokenize,pos')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0ELag21RARy"
      },
      "source": [
        "for ls in range(1, 18):\n",
        "    for file in range(92, 101):\n",
        "        stanza_documents: List[Document] = []\n",
        "        text_filename = '/content/drive/MyDrive/Lok Sabha Data/textfiles/' + str(ls_num) + '/' + str(file) + '.csv'\n",
        "        textfile = pd.read_csv(text_filename, lineterminator='\\n', names=['index','speech'])\n",
        "        if textfile['speech'].isnull().all():\n",
        "            print(str(ls_num) + '-' + str(file) + ' is empty!')\n",
        "        else:\n",
        "            speech = [x for x in textfile['speech'].values if x]\n",
        "            for document in batch(speech, nlp, batch_size=32):\n",
        "            stanza_documents.append(document)\n",
        "            pos = []\n",
        "            for i in range(len(stanza_documents)):\n",
        "                for j in range(len(stanza_documents[i].sentences)):\n",
        "                    for k in range(len(stanza_documents[i].sentences[j].words)):\n",
        "                        pos.append([i-1, j, k, stanza_documents[i].sentences[j].words[k].text, stanza_documents[i].sentences[j].words[k].upos, stanza_documents[i].sentences[j].words[k].xpos, stanza_documents[i].sentences[j].words[k].feats]) \n",
        "            POS = pd.DataFrame(pos, columns=['speech#', 'sent#', 'word#', 'word', 'word_upos', 'word_xpos', 'word_feats'])\n",
        "            POS_filename = '/content/drive/MyDrive/Lok Sabha Data/POS/' + str(ls_num) + '/' + str(file) + '_POS.csv'\n",
        "            POS.to_csv(POS_filename)\n",
        "            print(str(ls_num) + '-' + str(file) + ' POS done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uqkl1Ei1S7P6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}